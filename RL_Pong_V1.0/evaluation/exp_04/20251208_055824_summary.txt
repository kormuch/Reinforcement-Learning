============================================================
TRAINING SUMMARY
============================================================
Timestamp: 20251208_055824

TRAINING RESULTS
------------------------------------------------------------
Total episodes trained: 20000
Final running average reward: 1588.93
Best episode reward: 5401.18
Worst episode reward: -20.60
Mean episode reward: 730.38
Std episode reward: 1458.09

EVALUATION METRICS
------------------------------------------------------------
Win rate: 8.8%
Total wins: 1767 / 20000
Mean episode length: 4318 steps
Mean player score: 15.4
Mean CPU score: 20.9

REWARD COMPONENTS (Average)
------------------------------------------------------------
Score reward: +311.92
Hit reward: +0.9759
Loss penalty: -20.91

ACTION DISTRIBUTION
------------------------------------------------------------
N/A

HYPERPARAMETERS
------------------------------------------------------------
timestamp: 20251208_055824
experiment_name: exp_04
continued_from_episode: 0
environment: {'width': 160, 'height': 192, 'max_score': 21, 'cpu_difficulty': 'medium'}
agent: {'input_size': 16, 'hidden_size': 128, 'output_size': 3, 'total_params': 2563}
training: {'max_episodes': 20000, 'batch_size': 5, 'running_reward_decay': 0.99, 'print_frequency': 100}
rewards: {'ball_hit': 0.02, 'score': 20.0, 'opponent_score': -1.0, 'win': 5000.0, 'loss': -0.0}
training_results: {'final_running_average': 1683.1019917423907, 'best_episode_reward': 5401.18, 'total_episodes_trained': 20000, 'total_episodes_cumulative': 20000}
evaluation_results: {'mean_reward': 1872.2859999999996, 'std_reward': 2310.1593885626157, 'win_rate': 0.3, 'eval_episodes': 10}
heatmap_data: {'recording_enabled': False, 'heatmap_file': None, 'epochs_collected': 0}
collision_data: {'recording_enabled': True, 'epochs_collected': 100, 'total_collisions': 513881}

============================================================