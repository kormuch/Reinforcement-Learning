============================================================
TRAINING SUMMARY
============================================================
Timestamp: 20251208_124350

TRAINING RESULTS
------------------------------------------------------------
Total episodes trained: 20000
Final running average reward: 2720.69
Best episode reward: 5401.22
Worst episode reward: -20.60
Mean episode reward: 989.07
Std episode reward: 1734.22

EVALUATION METRICS
------------------------------------------------------------
Win rate: 13.4%
Total wins: 2670 / 20000
Mean episode length: 4375 steps
Mean player score: 17.1
Mean CPU score: 20.9

REWARD COMPONENTS (Average)
------------------------------------------------------------
Score reward: +343.85
Hit reward: +0.9930
Loss penalty: -20.87

ACTION DISTRIBUTION
------------------------------------------------------------
N/A

HYPERPARAMETERS
------------------------------------------------------------
timestamp: 20251208_124350
experiment_name: exp_05
continued_from_episode: 0
environment: {'width': 160, 'height': 192, 'max_score': 21, 'cpu_difficulty': 'medium'}
agent: {'input_size': 16, 'hidden_size': 128, 'output_size': 3, 'total_params': 2563}
training: {'max_episodes': 20000, 'batch_size': 5, 'running_reward_decay': 0.99, 'print_frequency': 100}
rewards: {'ball_hit': 0.02, 'score': 20.0, 'opponent_score': -1.0, 'win': 5000.0, 'loss': -0.0}
training_results: {'final_running_average': 2124.658101585911, 'best_episode_reward': 5401.219999999999, 'total_episodes_trained': 20000, 'total_episodes_cumulative': 20000}
evaluation_results: {'mean_reward': 3392.638, 'std_reward': 2459.7922971901507, 'win_rate': 0.6, 'eval_episodes': 10}
heatmap_data: {'recording_enabled': False, 'heatmap_file': None, 'epochs_collected': 0}
collision_data: {'recording_enabled': True, 'epochs_collected': 100, 'total_collisions': 547671}

============================================================