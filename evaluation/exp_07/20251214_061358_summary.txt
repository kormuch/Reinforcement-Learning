============================================================
TRAINING SUMMARY
============================================================
Timestamp: 20251214_061358

TRAINING RESULTS
------------------------------------------------------------
Total episodes trained: 20000
Final running average reward: 658.86
Best episode reward: 5401.20
Worst episode reward: -20.60
Mean episode reward: 558.48
Std episode reward: 1163.67

EVALUATION METRICS
------------------------------------------------------------
Win rate: 5.4%
Total wins: 1083 / 20000
Mean episode length: 4332 steps
Mean player score: 15.4
Mean CPU score: 20.9

REWARD COMPONENTS (Average)
------------------------------------------------------------
Score reward: +310.93
Hit reward: +0.9806
Loss penalty: -20.95

ACTION DISTRIBUTION
------------------------------------------------------------
N/A

HYPERPARAMETERS
------------------------------------------------------------
timestamp: 20251214_061358
experiment_name: exp_07
continued_from_episode: 0
environment: {'width': 160, 'height': 192, 'max_score': 21, 'cpu_difficulty': 'medium'}
agent: {'input_size': 16, 'hidden_size': 128, 'output_size': 3, 'total_params': 2563}
training: {'max_episodes': 20000, 'batch_size': 5, 'running_reward_decay': 0.99, 'print_frequency': 100}
rewards: {'ball_hit': 0.02, 'score': 20.0, 'opponent_score': -1.0, 'win': 5000.0, 'loss': -0.0}
training_results: {'final_running_average': 950.8728289834211, 'best_episode_reward': 5401.2, 'total_episodes_trained': 20000, 'total_episodes_cumulative': 20000}
evaluation_results: {'mean_reward': 356.0379999999999, 'std_reward': 27.981178602767898, 'win_rate': 0.0, 'eval_episodes': 10}
heatmap_data: {'recording_enabled': False, 'heatmap_file': None, 'epochs_collected': 0}
collision_data: {'recording_enabled': True, 'epochs_collected': 100, 'total_collisions': 515587}

============================================================